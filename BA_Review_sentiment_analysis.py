# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H2GlwWYKJgEItw_-Ysox1nsiDIiBmDjD
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
base_url = "https://www.airlinequality.com/airline-reviews/british-airways"
pages = 10
page_size = 100
reviews = []

# for i in range(1, pages + 1):
for i in range(1, pages + 1):
    print(f"Scraping page {i}")

    # Create URL to collect links from paginated data
    url = f"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}"

    response = requests.get(url)

    # Parse content
    content = response.content
    parsed_content = BeautifulSoup(content, 'html.parser')
    for para in parsed_content.find_all("div", {"class": "text_content"}):
        reviews.append(para.get_text())

    print(f"   ---> {len(reviews)} total reviews")
df = pd.DataFrame()
df["reviews"] = reviews
df.head()


!pip install nltk
import nltk
nltk.download('vader_lexicon')

df.to_csv("BA_reviews.csv")

from google.colab import drive
drive.mount('/content/drive')

df.to_csv("/content/drive/MyDrive/BA_reviews.csv")


#import os
#print(os.getcwd())
#print(os.listdir())
#os.mkdir("data")
#print(os.getcwd())
#df.to_csv("C:/Users/Haniish/data/BA_reviews.csv")

"""# New section"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')


review_data = pd.read_csv("/content/drive/MyDrive/BA_reviews.csv")
#review_data = pd.read_csv("https://drive.google.com/drive/my-drive/BA_reviews.csv")



review_data.head(20)
review_data.describe()

col_list = review_data.columns.tolist()
print(col_list)

review_data.columns = ( 'rev_verify' , 'rev_data' )

df = pd.DataFrame(review_data)
df
# Specify the set of words to be stripped off
words_to_strip = {'âœ… Trip Verified |', 'Not Verified |'}

# Create a new column by stripping off the words from the original column
df['verification'] = df['rev_data'].str.replace('|'.join(words_to_strip), '').str.strip()

col_list = df.columns.tolist()
print(col_list)
print(df)

import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('punkt')

# Download stopwords if not already downloaded
nltk.download('stopwords')

# Get the text column from the DataFrame
text_column = df.iloc[:, 2]

# Tokenize and remove stopwords for each document
stopword_list = set(stopwords.words('english'))

# List to store the processed documents
documents = []



for text in text_column:
    # Tokenize the text
    tokens = word_tokenize(text)

    # Remove stopwords
    filtered_tokens = [word for word in tokens if word not in string.punctuation]

    filtered_tokens1 = [word for word in filtered_tokens if word.lower() not in stopword_list]



  # Add the filtered tokens to the documents list
    documents.append(filtered_tokens1)

# Print the list of documents for demonstration

counter = 1
for doc in documents:
    counter = counter + 1
    print(doc)
print("completed all text")

#!pip install scikit-learn
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Convert the list of tokenized words into a list of strings representing preprocessed documents
preprocessed_documents = [' '.join(words) for words in documents]
print(preprocessed_documents)
# Create a CountVectorizer object to convert text into a matrix of token counts
vectorizer = CountVectorizer()

# Fit the vectorizer to the preprocessed documents and transform the documents into a document-term matrix
dtm = vectorizer.fit_transform(preprocessed_documents)



# Build the LDA model
num_topics = 10
lda_model = LatentDirichletAllocation(n_components=num_topics)
lda_model.fit(dtm)


feature_names = vectorizer.get_feature_names_out()
print("Feature Names:", feature_names)

for topic_idx, topic in enumerate(lda_model.components_):
    top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]
    print(f"Topic #{topic_idx + 1}: {', '.join(top_words)}")

import gensim
from gensim import corpora

id2word = corpora.Dictionary(documents)

print(id2word)
##Sentiment analysis
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')

# Initialize the VADER sentiment analyzer
sid = SentimentIntensityAnalyzer()

count_pos = 0
count_neg = 0
count_neu = 0
pos_doc = []
neg_doc = []
neu_doc = []
for tokens in documents:
    review = ' '.join(tokens)  # Join tokens into a single string
    sentiment_scores = sid.polarity_scores(review)
    sentiment = sentiment_scores['compound']

    print("Review:", review)
    print("Sentiment Score:", sentiment)

    if sentiment >= 0.05:
        count_pos = count_pos + 1
        pos_doc.append('['+review + ']')
        print("Sentiment: Positive")
    elif sentiment <= -0.05:
        count_neg = count_neg + 1
        neg_doc.append(review)
        print("Sentiment: Negative")
    else:
        count_neu = count_neu + 1
        neu_doc.append(review)
        print("Sentiment: Neutral")

    print("-----------------------")
counter = 0
for doc in neg_doc:
    counter = counter + 1
    print("negative review", str(counter), doc)
print("counter =" , counter)
print(neg_doc)

"""# New section

Word Cloud
"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt


## review has all the text joined as a string
# Create a WordCloud object
print(documents)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(documents)
# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""Word cloud with negative comments



"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt



print(neg_doc)
type(neg_doc)
neg_doc1 = ' '.join(neg_doc)
type(neg_doc1)



wordcloud = WordCloud(width=800, height=400, background_color='white').generate(neg_doc1)
# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""Word cloud on positive reviews"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt



print(pos_doc)
type(pos_doc)
pos_doc1 = ' '.join(pos_doc)
type(pos_doc1)



wordcloud = WordCloud(width=800, height=400, background_color='white').generate(pos_doc1)
# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()